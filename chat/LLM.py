import os
from dotenv import load_dotenv
from langgraph.graph import StateGraph,START,END
from langgraph.graph.message import AnyMessage,add_messages
from typing import Annotated
from langgraph.prebuilt import ToolNode ,tools_condition
from typing_extensions import TypedDict
from langchain_core.prompts import ChatPromptTemplate
from langgraph.checkpoint.memory import MemorySaver
import nest_asyncio
nest_asyncio.apply()

load_dotenv()

memory_saver = MemorySaver()


def get_qdrant_client():
    from qdrant_client import QdrantClient
    return QdrantClient(
        url=os.getenv("QDRANT_URL"),
        api_key=os.getenv("QDRANT_API_KEY")
    )

def get_gemini_llm():
    from langchain_google_genai import ChatGoogleGenerativeAI
    return ChatGoogleGenerativeAI(
        model="gemini-2.0-flash",
        temperature=0.2,
        google_api_key=os.environ.get("GOOGLE_API_KEY")
    )

def get_groq_llm():
    from langchain_groq import ChatGroq
    return ChatGroq(
        model="llama3-8b-8192",
        temperature=0.2,
        groq_api_key=os.environ.get("GROQ_API_KEY")
    )

def get_embedder():
    from langchain_google_genai import GoogleGenerativeAIEmbeddings
    return GoogleGenerativeAIEmbeddings(model="models/embedding-001")





# create a state graph for the LLM
class State(TypedDict):
    messages: Annotated[list[AnyMessage],add_messages]



# function that decide which tool to call

# making Rag tool
def qdrant_rag_tool(query: str) -> str:
    """
    Searches Dhruv Sharma's portfolio content using vector similarity from Qdrant.

    This function embeds the user's query and performs a semantic search against
    Dhruv's portfolio website/documents that have been uploaded to Qdrant.

    Args:
        query (str): The user's question about Dhruv Sharma (e.g., education, skills, projects).

    Returns:
        str: The most relevant content retrieved from the portfolio vector store.
    """

    from langchain_qdrant import QdrantVectorStore
    from langchain_core.messages import AIMessage

    # Get the Qdrant client and embedder
    qdrant_client = get_qdrant_client()
    embedder = get_embedder()

    # Initialize vector store
    qdrant = QdrantVectorStore(
        client=qdrant_client,
        collection_name="portfolio",
        embedding=embedder
    )

    # Perform similarity search
    docs = qdrant.similarity_search(query, k=5)

    # Collect relevant content
    context = [doc.page_content for doc in docs]

    # Join results into a single string (returning list of strings isn't helpful in most LLM chains)
    return "\n\n".join(context)



from langchain.tools import Tool
tools= [ Tool.from_function(
        qdrant_rag_tool,
        name="PortfolioRAGSearch",
        description="Search Dhruv Sharmaâ€™s portfolio (education, experience, projects, achievements, etc.) using semantic search."
    )]

system_prompt = """
You are the official assistant of Dhruv Sharma.

When the user says "you", assume they are referring to Dhruv Sharma, not you (the assistant).

If the user's question is about Dhruv Sharmaâ€™s marks, education, projects, experience, or background, you MUST use the PortfolioRAGSearch tool.

If no data is found in the portfolio, politely inform the user.

If the question is not related to Dhruv Sharma, politely explain that you only assist with questions about him.
"""


def run_llm_with_tools(state:State):

    from langchain.schema import SystemMessage, HumanMessage

    llm = get_groq_llm().bind_tools(tools)

    query = state['messages'][-1].content

    result = llm.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=query)
    ])


    return {
        "messages": [result] 
    }

# function for giving ans in formate
def format_response(state:State):
    llm = get_gemini_llm()
    human_prompt = """
You are Dhruv Sharmaâ€™s AI assistant.

Your job is to format answers for end users. You're given:
- The user's question
- An initial (raw) answer generated by an internal system (RAG)
Your task is to format this raw answer into a clean, user-friendly response.

### Guidelines:
1. Only output a **final, polished response** suitable for the user â€” do **not** include or refer to the original raw content.
2. Use a professional, helpful, and clear tone.
3. If any links are present, extract and move them to a **ðŸ”— Links** section at the end, using markdown format.
4. Do not include technical metadata, IDs, or debugging information.
5. Never mention "raw answer", "RAG", or "retrieved documents".

---

**User Question:**  
{question}

**Initial Answer (for internal use only):**  
{raw_answer}

---

âœ… Now write the **final response** to show to the user, starting below this line:

Formatted Response:
"""



    prompt = ChatPromptTemplate.from_template(human_prompt)


    chain = prompt | llm
    question = state['messages'][0].content
    raw_answer = state['messages'][-1].content


    result = chain.invoke({
        "question": question,
        "raw_answer": raw_answer
    })

    return {"messages": [result]}



# Define the state graph
graph = StateGraph(State)

# Define the nodes in the graph
graph.add_node("Run LLM with Tools",run_llm_with_tools)
graph.add_node("Format Response",format_response)
graph.add_node("tools",ToolNode(tools))

# Define the transitions between nodes
graph.add_edge(START, "Run LLM with Tools")
graph.add_conditional_edges("Run LLM with Tools",tools_condition)
graph.add_edge("tools", "Format Response")
graph.add_edge("Format Response", END)

main_graph = graph.compile(checkpointer=memory_saver)



